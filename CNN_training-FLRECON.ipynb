{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2016/12/16/00:06:37'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data')\n",
    "x_size, y_size = 28, 28\n",
    "n_classes = 10\n",
    "\n",
    "n_epochs = 1000\n",
    "original_ckpt_path = './models/MNIST_NLA_vanilla.ckpt'\n",
    "#ckpt_path = './models/MNIST_NLA_vanilla.ckpt'\n",
    "variables_file = './variables/scheme1_fr.npz'\n",
    "activations_file = './variables/scheme1_dr.npz'\n",
    "\n",
    "def timestamp():\n",
    "    d = datetime.datetime.now()\n",
    "    return d.strftime(\"%Y/%m/%d/%X\")\n",
    "\n",
    "timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_batch_iterator(x, y, batch_size):\n",
    "    n = x.shape[0]\n",
    "    assert n == y.shape[0]\n",
    "    \n",
    "    while True:\n",
    "        index = np.random.randint(n, size=batch_size)\n",
    "        x_batch, y_batch = x[index], y[index]\n",
    "        yield x_batch.copy(), y_batch.copy()\n",
    "        \n",
    "def batch_iterator(x, y, batch_size):\n",
    "    n = x.shape[0]\n",
    "    assert n == y.shape[0]\n",
    "    \n",
    "    for i in range(0, n, batch_size):\n",
    "        x_batch, y_batch = x[i:i+batch_size], y[i:i+batch_size]        \n",
    "        yield x_batch.copy(), y_batch.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_cnn(inputs, is_training, n_conv, conv_base, conv_mul,\n",
    "              conv_size, pool_size):\n",
    "    l = inputs\n",
    "    for i in range(n_conv):\n",
    "        n_filters = conv_base * conv_mul ** i\n",
    "        l = slim.conv2d(l, n_filters, [conv_size, conv_size],\n",
    "                        scope='conv{}'.format(i+1), is_training=is_training)\n",
    "        l = slim.max_pool2d(l, [pool_size, pool_size], scope='maxpool{}'.format(i+1), is_training=is_training)\n",
    "    l = slim.flatten(l)\n",
    "    \n",
    "    l = slim.dropout(l, 0.5, scope='dropout', is_training=is_training)\n",
    "    l = slim.fully_connected(l, 10, activation_fn=None, scope='logits', is_training=is_training)\n",
    "    return l\n",
    "\n",
    "def build_cnn_method1(inputs, is_training, n_conv, conv_base, conv_mul,\n",
    "              conv_size, pool_size, init_vals):\n",
    "    l = inputs\n",
    "    \n",
    "    for i in range(n_conv):\n",
    "        n_filters = conv_base * conv_mul ** i\n",
    "        M = n_filters // 2\n",
    "        \n",
    "        batch_size, w, h, in_channels = l.get_shape()\n",
    "        \n",
    "        with tf.variable_scope('conv{}'.format(i+1)):\n",
    "            b = init_vals[0][i]\n",
    "            init_val = tf.constant(b, dtype=tf.float32)\n",
    "            #f = tf.get_variable('basis', initializer=init_val,\n",
    "            #                   trainable=False)\n",
    "            l = tf.nn.depthwise_conv2d(l, init_val, [1, 1, 1, 1], padding='SAME', name='dpconv')\n",
    "            a = init_vals[1][i]\n",
    "            init_a = tf.constant(a, dtype=tf.float32)\n",
    "            #f1 = tf.get_variable('a1', initializer=init_a,\n",
    "             #                  trainable=False)\n",
    "            l = tf.nn.relu(tf.nn.conv2d(l, init_a, strides=[1, 1, 1, 1], padding='SAME'))\n",
    "            \n",
    "        l = slim.max_pool2d(l, [pool_size, pool_size], scope='maxpool{}'.format(i+1))\n",
    "    l = slim.flatten(l)\n",
    "    \n",
    "    l = slim.dropout(l, 0.5, scope='dropout', is_training=is_training)\n",
    "    l = slim.fully_connected(l, 10, activation_fn=None, scope='logits', trainable=False)\n",
    "    return l\n",
    "\n",
    "def build_loss(logits, y_true):\n",
    "    logloss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y_true),\n",
    "                             name='logloss')\n",
    "    return logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "test_list = list(batch_iterator(\n",
    "        mnist.test.images, mnist.test.labels, batch_size=batch_size))\n",
    "\n",
    "n_conv = 2\n",
    "conv_base = 32\n",
    "conv_mul = 2\n",
    "conv_size = 5\n",
    "pool_size = 2\n",
    "\n",
    "def build_model(graph, build_cnn, init_vals):\n",
    "    with graph.as_default():#, graph.device('/cpu:0'):\n",
    "        with tf.variable_scope('model') as vs:\n",
    "            is_training = tf.placeholder(tf.bool)\n",
    "            x_ph = tf.placeholder(tf.float32, shape=[batch_size, x_size * y_size])\n",
    "            x_image = tf.reshape(x_ph, [-1, x_size, y_size, 1])\n",
    "            y_ph = tf.placeholder(tf.int64, shape=[batch_size])\n",
    "            \n",
    "            logits = build_cnn(x_image, is_training=is_training, n_conv=n_conv,\n",
    "                               conv_base=conv_base, conv_mul=conv_mul,\n",
    "                               conv_size=conv_size, pool_size=pool_size, init_vals=init_vals)\n",
    "            \n",
    "            prediction = tf.nn.softmax(logits, name='predictions')\n",
    "\n",
    "            loss = build_loss(logits, y_ph)\n",
    "\n",
    "            #optimizer = tf.train.AdamOptimizer().minimize(loss, name='optimizer')\n",
    "\n",
    "            correct_prediction = tf.equal(tf.argmax(prediction, 1), y_ph)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "\n",
    "        # Code to use of tensorboard\n",
    "        with tf.name_scope('summaries'):\n",
    "            tf.scalar_summary('log_loss', loss)\n",
    "            tf.scalar_summary('acc', accuracy)\n",
    "            merged_summary = tf.merge_all_summaries()\n",
    "            \n",
    "    return {\n",
    "        'is_training': is_training,\n",
    "        'x_ph': x_ph,\n",
    "        'y_ph': y_ph,\n",
    "        'prediction': prediction,\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy,\n",
    "        'merged_summary': merged_summary\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_net(layers, session, n_epochs, tb_path='/tmp/tf/', ckpt=None):\n",
    "    tb_path = tb_path + timestamp()\n",
    "    \n",
    "    l = layers\n",
    "    x_ph, y_ph, is_training = l['x_ph'], l['y_ph'], l['is_training']\n",
    "    val_operations = [l['merged_summary'], l['accuracy'], l['loss']]\n",
    "    train_operations = [l['optimizer']] + val_operations\n",
    "    \n",
    "    train_iterator = random_batch_iterator(\n",
    "        mnist.train.images, mnist.train.labels, batch_size=batch_size)\n",
    "    val_iterator = random_batch_iterator(\n",
    "        mnist.validation.images, mnist.validation.labels, batch_size=batch_size)\n",
    "        \n",
    "    train_writer = tf.train.SummaryWriter(tb_path+'/train', session.graph)\n",
    "    val_writer = tf.train.SummaryWriter(tb_path+'/val', session.graph)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    \n",
    "    variables_to_restore = slim.get_variables_to_restore(exclude=[\"model/conv1\", \"model/conv2\", \n",
    "                                                                  \"model/model/conv1\", \"model/model/conv2\"])\n",
    "    #print(variables_to_restore)\n",
    "    print([v.name for v in variables_to_restore])\n",
    "    restore = tf.train.Saver(variables_to_restore)\n",
    "    restore.restore(session, ckpt)\n",
    "    #to_drop = False\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(n_epochs):\n",
    "        x_batch, y_batch = next(train_iterator)\n",
    "        feed_dict = {x_ph: x_batch, y_ph: y_batch, is_training: True}\n",
    "        _, summary, acc, _ = session.run(train_operations, feed_dict)\n",
    "        train_writer.add_summary(summary, epoch)\n",
    "\n",
    "        x_batch, y_batch = next(val_iterator)\n",
    "        feed_dict = {x_ph: x_batch, y_ph: y_batch, is_training: False}\n",
    "        summary, acc, _ = session.run(val_operations, feed_dict)\n",
    "        val_writer.add_summary(summary, epoch)\n",
    "        print(acc)\n",
    "                \n",
    "    return best_acc\n",
    "\n",
    "def evaluate_net(layers, session, ckpt):\n",
    "    l = layers\n",
    "    x_ph, y_ph, is_training = l['x_ph'], l['y_ph'], l['is_training']\n",
    "    \n",
    "    test_iterator = iter(test_list)\n",
    "    \n",
    "    n, test_acc = 0, 0.0\n",
    "    variables_to_restore = slim.get_variables_to_restore(exclude=[\"model/conv1\", \"model/conv2\", \n",
    "                                                                  \"model/model/conv1\", \"model/model/conv2\"])\n",
    "    #print(variables_to_restore)\n",
    "    print([v.name for v in variables_to_restore])\n",
    "    restore = tf.train.Saver(variables_to_restore)\n",
    "    restore.restore(session, ckpt)\n",
    "    start = time.time()\n",
    "    for x_batch, y_batch in test_iterator:\n",
    "        if len(x_batch) != batch_size:\n",
    "            break\n",
    "        feed_dict = {x_ph: x_batch, y_ph: y_batch, is_training: False}\n",
    "        test_acc += l['accuracy'].eval(feed_dict=feed_dict)\n",
    "        n += 1\n",
    "    end = time.time()\n",
    "    test_acc = test_acc / n\n",
    "    return test_acc, end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'model/logits/weights:0', u'model/logits/biases:0']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.24168669871794871, 45.646302938461304)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "init_vals = [[np.load(\"trial_conv1.npy\"), np.load(\"trial_conv2.npy\")],\n",
    "             [np.load(\"trial_a1.npy\"), np.load(\"trial_a2.npy\")]]\n",
    "layers = build_model(graph, build_cnn_method1, init_vals)\n",
    "with tf.Session(graph=graph) as session:\n",
    "    test_acc, els = evaluate_net(layers, session, ckpt=original_ckpt_path)\n",
    "\n",
    "test_acc, els"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    restorer = tf.train.Saver()\n",
    "    restorer.restore(session, ckpt_path)\n",
    "    conv_vars = {}\n",
    "    for i in range(n_conv):\n",
    "        for name in ['weights', 'bias']:\n",
    "            full_name = 'conv{}/{}'.format(i+1, name)\n",
    "            conv_vars[full_name] = slim.get_variables(scope='model/'+full_name)[0].eval()\n",
    "            \n",
    "np.savez(variables_file, **conv_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.load(variables_file).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_measurements = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    restorer = tf.train.Saver()\n",
    "    restorer.restore(session, ckpt_path)\n",
    "    mesurements = []\n",
    "    for i in range(n_measurements):\n",
    "        test_acc, els = evaluate_net(layers, session)\n",
    "        mesurements.append(els)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(mesurements, bins=40, range=(0.135, 0.15))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
